{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import argparse\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from tokenization import BertTokenizer\n",
    "from modeling import BertModel, BertPreTrainedModel, BertLayer, BertPooler\n",
    "from optimization import BertAdam\n",
    "from prettytable import PrettyTable\n",
    "import absa_data_utils as data_utils\n",
    "from absa_data_utils import ABSATokenizer\n",
    "import modelconfig\n",
    "from math import ceil\n",
    "from layer import PrimaryCaps, FCCaps, FlattenCaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warmup_linear(x, warmup=0.002):\n",
    "    if x < warmup:\n",
    "        return x/warmup\n",
    "    return 1.0 - x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapsLayer(nn.Module):\n",
    "    def __init__(self, config, num_labels):\n",
    "        super(CapsLayer, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.ngram_size = [2,4,8]\n",
    "        self.convs_doc = nn.ModuleList([nn.Conv1d(config.max_seq_length, 32, K, stride=2) for K in self.ngram_size])\n",
    "        torch.nn.init.xavier_uniform_(self.convs_doc[0].weight)\n",
    "        torch.nn.init.xavier_uniform_(self.convs_doc[1].weight)\n",
    "        torch.nn.init.xavier_uniform_(self.convs_doc[2].weight)\n",
    "        self.primary_capsules_doc = PrimaryCaps(num_capsules=config.dim_capsule, in_channels=32, \n",
    "                                                out_channels=32, kernel_size=1, stride=1)\n",
    "        self.flatten_capsules = FlattenCaps()\n",
    "        self.W_doc = nn.Parameter(torch.FloatTensor(36736, config.num_compressed_capsule))\n",
    "        torch.nn.init.xavier_uniform_(self.W_doc)\n",
    "\n",
    "        self.fc_capsules_doc_child = FCCaps(config, \n",
    "                                            output_capsule_num=self.num_labels, \n",
    "                                            input_capsule_num=config.num_compressed_capsule, \n",
    "                                            in_channels=config.dim_capsule, out_channels=config.dim_capsule)\n",
    "        \n",
    "    def compression(self, poses, W):\n",
    "        poses = torch.matmul(poses.permute(0,2,1), W).permute(0,2,1)\n",
    "        activations = torch.sqrt((poses ** 2).sum(2))\n",
    "        return poses, activations\n",
    "        \n",
    "    def forward(self, embs, labels):\n",
    "        nets_doc_l = []\n",
    "        for i in range(len(self.ngram_size)):\n",
    "            nets = self.convs_doc[i](embs)\n",
    "            nets_doc_l.append(nets)\n",
    "        nets_doc = torch.cat((nets_doc_l[0], nets_doc_l[1], nets_doc_l[2]), 2)\n",
    "        poses_doc, activations_doc = self.primary_capsules_doc(nets_doc)\n",
    "        poses, activations = self.flatten_capsules(poses_doc, activations_doc)\n",
    "        poses, activations = self.compression(poses, self.W_doc)\n",
    "        poses, activations = self.fc_capsules_doc_child(poses, activations, labels)\n",
    "        return poses, activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForABSA(BertPreTrainedModel):\n",
    "    def __init__(self, config, num_labels=3):\n",
    "        super(BertForABSA, self).__init__(config)\n",
    "        self.num_labels = num_labels\n",
    "        self.bert = BertModel(config)\n",
    "        self.capsLayer = CapsLayer(config, num_labels)\n",
    "        self.classifier = torch.nn.Linear(16, num_labels)\n",
    "        self.loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
    "        self.apply(self.init_bert_weights)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
    "        layers, _, mask = self.bert(input_ids, token_type_ids, \n",
    "                                                        attention_mask=attention_mask, \n",
    "                                                        output_all_encoded_layers=True)\n",
    "        poses, activations = self.capsLayer(layers[-1], labels)\n",
    "        activations = activations.squeeze(2)\n",
    "        print(activations.shape)\n",
    "        logits = self.classifier(activations)\n",
    "        loss = self.loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        return loss, logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "\n",
    "    processor = data_utils.AscProcessor()\n",
    "    label_list = processor.get_labels()\n",
    "    tokenizer = ABSATokenizer.from_pretrained(modelconfig.MODEL_ARCHIVE_MAP[args.bert_model])\n",
    "    train_examples = processor.get_train_examples(args.data_dir)\n",
    "    num_train_steps = int(len(train_examples) / args.train_batch_size) * args.num_train_epochs\n",
    "\n",
    "    train_features = data_utils.convert_examples_to_features(\n",
    "        train_examples, label_list, args.max_seq_length, tokenizer, \"asc\")\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_examples))\n",
    "    logger.info(\"  Batch size = %d\", args.train_batch_size)\n",
    "    logger.info(\"  Num steps = %d\", num_train_steps)\n",
    "    \n",
    "    all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
    "    all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)\n",
    "    \n",
    "    train_data = TensorDataset(all_input_ids, all_segment_ids, all_input_mask, all_label_ids)\n",
    "\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.train_batch_size)\n",
    "    \n",
    "    #>>>>> validation\n",
    "    if args.do_valid:\n",
    "        valid_examples = processor.get_dev_examples(args.data_dir)\n",
    "        valid_features=data_utils.convert_examples_to_features(\n",
    "            valid_examples, label_list, args.max_seq_length, tokenizer, \"asc\")\n",
    "        valid_all_input_ids = torch.tensor([f.input_ids for f in valid_features], dtype=torch.long)\n",
    "        valid_all_segment_ids = torch.tensor([f.segment_ids for f in valid_features], dtype=torch.long)\n",
    "        valid_all_input_mask = torch.tensor([f.input_mask for f in valid_features], dtype=torch.long)\n",
    "        valid_all_label_ids = torch.tensor([f.label_id for f in valid_features], dtype=torch.long)\n",
    "        valid_data = TensorDataset(valid_all_input_ids, valid_all_segment_ids, valid_all_input_mask, valid_all_label_ids)\n",
    "\n",
    "        logger.info(\"***** Running validations *****\")\n",
    "        logger.info(\"  Num orig examples = %d\", len(valid_examples))\n",
    "        logger.info(\"  Num split examples = %d\", len(valid_features))\n",
    "        logger.info(\"  Batch size = %d\", args.train_batch_size)\n",
    "\n",
    "        valid_sampler = SequentialSampler(valid_data)\n",
    "        valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=args.train_batch_size)    \n",
    "\n",
    "        best_valid_loss=float('inf')\n",
    "        valid_losses=[]\n",
    "    #<<<<< end of validation declaration\n",
    "\n",
    "    model = BertForABSA.from_pretrained(modelconfig.MODEL_ARCHIVE_MAP[args.bert_model], num_labels=len(label_list))\n",
    "    model.cuda()\n",
    "    \n",
    "    # Prepare optimizer\n",
    "    param_optimizer = [(k, v) for k, v in model.named_parameters() if v.requires_grad==True]\n",
    "    param_optimizer = [n for n in param_optimizer if 'pooler' not in n[0]]\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "    t_total = num_train_steps\n",
    "    optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                         lr=args.learning_rate,\n",
    "                         warmup=args.warmup_proportion,\n",
    "                         t_total=t_total)\n",
    "    \n",
    "    ########################################################\n",
    "    # Freeze bert\n",
    "    for param in model.bert.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Count number of parameters\n",
    "    def count_parameters(model):\n",
    "        table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "        total_params = 0\n",
    "        for name, parameter in model.named_parameters():\n",
    "            if not parameter.requires_grad: continue\n",
    "            param = parameter.numel()\n",
    "            table.add_row([name, param])\n",
    "            total_params+=param\n",
    "        print(table)\n",
    "        print(f\"Total Trainable Params: {total_params}\")\n",
    "        return total_params\n",
    "    \n",
    "    count_parameters(model)\n",
    "    ########################################################\n",
    "    global_step = 0\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(args.num_train_epochs):\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch = tuple(t.cuda() for t in batch)\n",
    "            input_ids, segment_ids, input_mask, label_ids = batch\n",
    "            # input_ids: [batch_size, sequence_length]\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss, _ = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "            loss.backward()\n",
    "            \n",
    "            lr_this_step = args.learning_rate * warmup_linear(global_step/t_total, args.warmup_proportion)\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr_this_step\n",
    "            optimizer.step()\n",
    "            global_step += 1\n",
    "        print(\"training loss: \", loss.item(), epoch+1)\n",
    "        #>>>> perform validation at the end of each epoch.\n",
    "        new_dirs = os.path.join(args.output_dir, str(epoch+1))\n",
    "        os.mkdir(new_dirs)\n",
    "        if args.do_valid:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                losses=[]\n",
    "                valid_size=0\n",
    "                for step, batch in enumerate(valid_dataloader):\n",
    "                    batch = tuple(t.cuda() for t in batch) # multi-gpu does scattering it-self\n",
    "                    input_ids, segment_ids, input_mask, label_ids = batch\n",
    "                    loss, _ = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "                    losses.append(loss.data.item()*input_ids.size(0) )\n",
    "                    valid_size+=input_ids.size(0)\n",
    "                valid_loss=sum(losses)/valid_size\n",
    "                logger.info(\"validation loss: %f, epoch: %d\", valid_loss, epoch+1)\n",
    "                valid_losses.append(valid_loss)\n",
    "                torch.save(model, os.path.join(new_dirs, \"model.pt\"))\n",
    "                test(args, new_dirs, dev_as_test=True)\n",
    "                if epoch == args.num_train_epochs-1:\n",
    "                    torch.save(model, os.path.join(args.output_dir, \"model.pt\"))\n",
    "                    test(args, args.output_dir, dev_as_test=False)\n",
    "                os.remove(os.path.join(new_dirs, \"model.pt\"))\n",
    "            if valid_loss<best_valid_loss:\n",
    "                best_valid_loss=valid_loss\n",
    "            model.train()\n",
    "    if args.do_valid:\n",
    "        with open(os.path.join(args.output_dir, \"valid.json\"), \"w\") as fw:\n",
    "            json.dump({\"valid_losses\": valid_losses}, fw)\n",
    "    else:\n",
    "        torch.save(model, os.path.join(args.output_dir, \"model.pt\") )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():    \n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\"--bert_model\", default='rest_pt', type=str)\n",
    "\n",
    "    parser.add_argument(\"--data_dir\",\n",
    "                        default='../asc/rest/',\n",
    "                        type=str,\n",
    "                        required=False,\n",
    "                        help=\"The input data dir containing json files.\")\n",
    "\n",
    "    parser.add_argument(\"--output_dir\",\n",
    "                        default='../run/pt_asc/rest/',\n",
    "                        type=str,\n",
    "                        required=False,\n",
    "                        help=\"The output directory where the model predictions and checkpoints will be written.\")\n",
    "\n",
    "    ## Other parameters\n",
    "    parser.add_argument(\"--max_seq_length\",\n",
    "                        default=100,\n",
    "                        type=int,\n",
    "                        help=\"The maximum total input sequence length after WordPiece tokenization. \\n\"\n",
    "                             \"Sequences longer than this will be truncated, and sequences shorter \\n\"\n",
    "                             \"than this will be padded.\")\n",
    "    parser.add_argument(\"--do_train\",\n",
    "                        default=True,\n",
    "                        action='store_true',\n",
    "                        help=\"Whether to run training.\")\n",
    "    parser.add_argument(\"--do_valid\",\n",
    "                        default=False,\n",
    "                        action='store_true',\n",
    "                        help=\"Whether to run training.\")\n",
    "    parser.add_argument(\"--do_eval\",\n",
    "                        default=False,\n",
    "                        action='store_true',\n",
    "                        help=\"Whether to run eval on the dev set.\")\n",
    "    \n",
    "    parser.add_argument(\"--train_batch_size\",\n",
    "                        default=16,\n",
    "                        type=int,\n",
    "                        help=\"Total batch size for training.\")\n",
    "    parser.add_argument(\"--eval_batch_size\",\n",
    "                        default=8,\n",
    "                        type=int,\n",
    "                        help=\"Total batch size for eval.\")\n",
    "    parser.add_argument(\"--learning_rate\",\n",
    "                        default=3e-5,\n",
    "                        type=float,\n",
    "                        help=\"The initial learning rate for Adam.\")\n",
    "    \n",
    "    parser.add_argument(\"--num_train_epochs\",\n",
    "                        default=4,\n",
    "                        type=int,\n",
    "                        help=\"Total number of training epochs to perform.\")\n",
    "    parser.add_argument(\"--warmup_proportion\",\n",
    "                        default=0.1,\n",
    "                        type=float,\n",
    "                        help=\"Proportion of training to perform linear learning rate warmup for. \"\n",
    "                             \"E.g., 0.1 = 10%% of training.\")\n",
    "    parser.add_argument('--seed',\n",
    "                        type=int,\n",
    "                        default=1,\n",
    "                        help=\"random seed for initialization\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "    if args.do_train:\n",
    "        train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--bert_model BERT_MODEL]\n",
      "                             [--data_dir DATA_DIR] [--output_dir OUTPUT_DIR]\n",
      "                             [--max_seq_length MAX_SEQ_LENGTH] [--do_train]\n",
      "                             [--do_valid] [--do_eval]\n",
      "                             [--train_batch_size TRAIN_BATCH_SIZE]\n",
      "                             [--eval_batch_size EVAL_BATCH_SIZE]\n",
      "                             [--learning_rate LEARNING_RATE]\n",
      "                             [--num_train_epochs NUM_TRAIN_EPOCHS]\n",
      "                             [--warmup_proportion WARMUP_PROPORTION]\n",
      "                             [--seed SEED]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/akbar/.local/share/jupyter/runtime/kernel-d8415329-2384-4c10-91b5-526d95b0e3f5.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ../src/run_$task.py \\\n",
    "            --bert_model $bert --do_train --do_valid \\\n",
    "            --max_seq_length 100 --train_batch_size 16 --learning_rate 3e-5 --num_train_epochs 4 \\\n",
    "            --output_dir $OUTPUT_DIR --data_dir $DATA_DIR --seed $run"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python37464bitbaseconda33ddd304cc884fad8ef44ad940b23978"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
